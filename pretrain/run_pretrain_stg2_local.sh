#!/bin/bash

# ================= é…ç½®åŒºåŸŸ =================
# æŒ‡å®šä½¿ç”¨çš„ GPU ç¼–å·
export CUDA_VISIBLE_DEVICES=2,3,4,5

# Stage 1 è¾“å‡ºçš„ Converted æ¨¡å‹è·¯å¾„
# æ³¨æ„ï¼šè¿è¡Œæ­¤è„šæœ¬å‰ï¼Œè¯·ç¡®ä¿å·²è¿è¡Œ checkpoint conversion è„šæœ¬å°† Stage 1 ç»“æœè½¬æ¢ä¸º HF æ ¼å¼
# ä¾‹å¦‚ï¼šbash scripts/convert_checkpoint_to_hf.sh <BASE_MODEL_PATH> <STAGE1_OUTPUT_DIR> <STEP>
STAGE1_OUTPUT_DIR="/zhdd/home/lkzhang/vscode/evaluate_exp/OpenOneRec/output/stg1_opt"
# å‡è®¾ Stage 1 è·‘äº† 2000 æ­¥å¹¶å·²è½¬æ¢
MODEL_DIR="${STAGE1_OUTPUT_DIR}/step2000/global_step2000/converted"

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR="/zhdd/home/lkzhang/vscode/evaluate_exp/OpenOneRec/output/stg2_opt"

# æ•°æ®é…ç½®
DATASET_CONFIG="examples/dataset_config/pretrain.json"

# ===========================================

mkdir -p $OUTPUT_DIR
mkdir -p /tmp/_wids_cache

export PYTHONPATH=$PWD:$PYTHONPATH

# Disable proxy for localhost to avoid torch.distributed connection issues
export no_proxy=localhost,127.0.0.1,::1
export NO_PROXY=localhost,127.0.0.1,::1

# è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒçš„ç¯å¢ƒå˜é‡
export MASTER_ADDR=localhost
export MASTER_PORT=29501 # ä½¿ç”¨ä¸åŒçš„ç«¯å£ä»¥é˜²å†²çª

echo "ğŸš€ å¼€å§‹ Stage 2 é¢„è®­ç»ƒ..."
echo "ğŸ“ åŠ è½½ Stage 1 æ¨¡å‹: $MODEL_DIR"
echo "ğŸ“ è¾“å‡º: $OUTPUT_DIR"

if [ ! -d "$MODEL_DIR" ]; then
    echo "âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ç›®å½• $MODEL_DIR"
    echo "è¯·æ£€æŸ¥è·¯å¾„ï¼Œæˆ–ç¡®è®¤æ˜¯å¦å·²è¿è¡Œ checkpoint è½¬æ¢è„šæœ¬ã€‚"
    exit 1
fi

# Unset LD_PRELOAD to disable proxychains for torchrun
unset LD_PRELOAD

# ä½¿ç”¨ torchrun å¯åŠ¨
# nproc_per_node = 4 (å¯¹åº”ä¸Šé¢ CUDA_VISIBLE_DEVICES çš„æ•°é‡)
/home/lkzhang/miniconda3/envs/openonerec/bin/torchrun --nproc_per_node=4 \
    --master_port=$MASTER_PORT \
    recipes/train_qwen3.py \
    --model_dir $MODEL_DIR \
    --output_dir $OUTPUT_DIR \
    --dataset_config $DATASET_CONFIG \
    --use_tie_weights \
    --model_class Qwen3ForCausalLM \
    --monitor_datasource_loss \
    --monitor_datasource_cnt \
    --max_length 32768 \
    --learning_rate 2e-4 \
    --min_lr 1e-4 \
    --weight_decay 0.1 \
    --lr_scheduler_type cosine \
    --num_warmup_steps 500 \
    --num_training_steps 5000 \
    --save_checkpoint_per_step 50 \
    --minibatch_size 4096 \
    --logging_per_step 5 \
    --use_fp32_weight \
    --seed 19260817 \
    --enable_gradient_checkpointing \
    --use_chunked_loss_computer
